{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "# Module Importations\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Module Versioning\n",
    "print(sklearn.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Modules\n",
    "from Source.Data import ntfp_dataset_import as data_load\n",
    "from Source.Data import ntfp_split_data as split_data\n",
    "from Source.Features import ntfp_dataset_preprocessing as preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MEAN_RUL = 107.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading pickled dataframe started ...\nLoading pickled dataframe complete ...\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 20631 entries, 1 to 100\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Cycles  20631 non-null  int64  \n 1   Sn_02   20631 non-null  float64\n 2   Sn_03   20631 non-null  float64\n 3   Sn_04   20631 non-null  float64\n 4   Sn_07   20631 non-null  float64\n 5   Sn_09   20631 non-null  float64\n 6   Sn_11   20631 non-null  float64\n 7   Sn_12   20631 non-null  float64\n 8   Sn_14   20631 non-null  float64\n 9   Sn_15   20631 non-null  float64\n 10  Sn_17   20631 non-null  int64  \n 11  Sn_20   20631 non-null  float64\n 12  Sn_21   20631 non-null  float64\n 13  RUL     20631 non-null  float64\ndtypes: float64(12), int64(2)\nmemory usage: 2.4 MB\nNone\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "data_filename = 'rul_dataset.pkl'\n",
    "rul_df = data_load.load_pickled_data(data_filename)\n",
    "\n",
    "print(rul_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.72172539 -0.13425518 -0.92593596 ... -0.78170979  1.34849274\n   1.19442705]\n [-1.06177971  0.21152849 -0.64372587 ... -0.78170979  1.01652793\n   1.23692196]\n [-0.66181262 -0.41316559 -0.52595315 ... -2.07309423  0.73989059\n   0.50342281]\n ...\n [ 1.47801126  1.94697106  2.13837684 ...  3.09244354 -2.08181033\n  -3.29248147]\n [ 1.09804254  2.40366648  1.95505138 ...  1.15536688 -2.91172236\n  -2.08507166]\n [ 2.33794049  1.60771161  2.57835793 ...  1.8010591  -2.46910261\n  -2.19408035]]\nShape: (20631, 12)\n"
     ]
    }
   ],
   "source": [
    "# Normalise columns into array\n",
    "normalised_array = preprocessing.standardise_columns(rul_df)\n",
    "\n",
    "print(normalised_array)\n",
    "print(\"Shape:\", normalised_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            Sn_02       Sn_03       Sn_04       Sn_07       Sn_09       Sn_11  \\\n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
      "mean     0.010975    0.010625    0.012324   -0.011917    0.007100    0.012641   \n",
      "std      0.002842    0.002459    0.003010    0.003444    0.009205    0.003387   \n",
      "min      0.005116    0.005192    0.007006   -0.020537   -0.005766    0.006454   \n",
      "25%      0.009114    0.008862    0.010195   -0.013892   -0.000709    0.010143   \n",
      "50%      0.010601    0.010617    0.012143   -0.011197    0.003977    0.012485   \n",
      "75%      0.012328    0.012220    0.013796   -0.009588    0.014261    0.014375   \n",
      "max      0.018116    0.016361    0.019765   -0.005189    0.026665    0.021425   \n",
      "\n",
      "            Sn_12       Sn_14       Sn_15       Sn_17       Sn_20       Sn_21  \n",
      "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000  \n",
      "mean    -0.012249    0.005574    0.011670    0.010988   -0.011533   -0.011540  \n",
      "std      0.003736    0.009765    0.002884    0.002383    0.002899    0.002842  \n",
      "min     -0.022150   -0.009362    0.006841    0.006122   -0.019988   -0.019546  \n",
      "25%     -0.014874   -0.002402    0.009384    0.009305   -0.013105   -0.013138  \n",
      "50%     -0.011462    0.003012    0.011378    0.010961   -0.011151   -0.011283  \n",
      "75%     -0.009409    0.012886    0.013416    0.012324   -0.009329   -0.009657  \n",
      "max     -0.005428    0.025416    0.019327    0.016854   -0.006681   -0.005770  \n"
     ]
    }
   ],
   "source": [
    "# Calculate slopes for each column\n",
    "slopes_df, slopes_array = preprocessing.calculate_slopes_all_engines(rul_df, normalised_array)\n",
    "\n",
    "print(slopes_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Slope Order: \n['Sn_11' 'Sn_04' 'Sn_12' 'Sn_07' 'Sn_15' 'Sn_21' 'Sn_20' 'Sn_17' 'Sn_02'\n 'Sn_03' 'Sn_09' 'Sn_14']\n"
     ]
    }
   ],
   "source": [
    "# Order slopes by value\n",
    "slope_order = preprocessing.return_data_ordered_abs_value(slopes_array, rul_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise slopes for each reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not influential: ['Sn_21' 'Sn_20' 'Sn_17' 'Sn_02' 'Sn_03' 'Sn_09' 'Sn_14']\n"
     ]
    }
   ],
   "source": [
    "# Determine [5] most influential columns\n",
    "data_columns = rul_df.columns.values[1:-1]\n",
    "\n",
    "slope_slice = slope_order[5:]\n",
    "\n",
    "data_columns_not_influential = data_columns[slope_slice]\n",
    "\n",
    "print(\"Not influential:\", data_columns_not_influential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        Cycles    Sn_04   Sn_07  Sn_11   Sn_12   Sn_15    RUL\nEngine                                                       \n1            1  1400.60  554.36  47.47  521.66  8.4195 -191.0\n1            2  1403.14  553.75  47.49  522.28  8.4318 -190.0\n1            3  1404.20  554.26  47.27  522.42  8.4178 -189.0\n1            4  1401.87  554.45  47.13  522.86  8.3682 -188.0\n1            5  1406.22  554.00  47.28  522.19  8.4294 -187.0\n...        ...      ...     ...    ...     ...     ...    ...\n100        196  1428.63  551.43  48.07  519.49  8.4956   -4.0\n100        197  1433.58  550.86  48.04  519.68  8.5139   -3.0\n100        198  1428.18  550.94  48.09  520.01  8.5646   -2.0\n100        199  1426.53  550.68  48.39  519.67  8.5389   -1.0\n100        200  1432.14  550.79  48.20  519.30  8.5036    0.0\n\n[20631 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove least influential columns\n",
    "rul_df = preprocessing.dataset_remove_columns(rul_df, data_columns_not_influential)\n",
    "\n",
    "print(rul_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pickling dataframe ...\nPickled dataframe to: C:/Developer/nasa-turbofan-failure-prediction/Data/Interim/rul_dataset_preprocessed.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save dataset\n",
    "filename = 'rul_dataset_preprocessed'\n",
    "data_load.pickle_data(rul_df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Data Items: 20631\nTraining Data Items: 16505\nEvaluation Data Items: 4126\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and evaluation sets\n",
    "training_set, evaluation_set = split_data.split_train_eval(rul_df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  88.     1410.13    552.95     47.65    521.17      8.4184]\n [  24.     1406.22    552.91     47.43    521.53      8.4314]\n [ 116.     1404.9     554.3      47.42    522.76      8.3808]\n ...\n [  82.     1415.54    553.5      47.48    521.68      8.3974]\n [  14.     1396.28    554.52     47.26    522.19      8.4214]\n [  42.     1401.35    554.01     47.12    521.91      8.3522]]\n[-187. -167. -115. ...  -74. -255. -157.]\n"
     ]
    }
   ],
   "source": [
    "# Create RUL Target Dataset\n",
    "\n",
    "# Training Set as Array\n",
    "rul_training_data = training_set.drop('RUL', axis = 1).values\n",
    "rul_training_label = training_set['RUL'].copy().values\n",
    "\n",
    "print(rul_training_data)\n",
    "print(rul_training_label)\n",
    "\n",
    "# Evaluation Set as Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LinReg_RUL rmse mean (cv): 41.18492574232157\nLinReg_RUL rmse std (cv): 0.5652551597479265\nLinReg_RUL mae mean (cv): 31.591276452712655\nLinReg_RUL mae std (cv): 0.4205946666188317\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Model (Target - RUL)\n",
    "lin_reg_rul = LinearRegression()\n",
    "model_name = \"LinReg_RUL\"\n",
    "\n",
    "# Train model\n",
    "lin_reg_rul.fit(rul_training_data, rul_training_label)\n",
    "\n",
    "# Save model\n",
    "\n",
    "# Compute RMSE via cross validation\n",
    "scores_mse = cross_val_score(lin_reg_rul, rul_training_data, rul_training_label, scoring = \"neg_mean_squared_error\", cv = 5)\n",
    "scores_rmse = np.sqrt(-scores_mse)\n",
    "\n",
    "# Compute MAE via cross validation\n",
    "scores_mae = cross_val_score(lin_reg_rul, rul_training_data, rul_training_label, scoring = \"neg_mean_absolute_error\", cv = 5)\n",
    "scores_mae = -1 * scores_mae\n",
    "\n",
    "# Visualise cross validation results\n",
    "print(model_name, \"rmse mean (cv):\", scores_rmse.mean())\n",
    "print(model_name, \"rmse std (cv):\", scores_rmse.std())\n",
    "\n",
    "print(model_name, \"mae mean (cv):\", scores_mae.mean())\n",
    "print(model_name, \"mae std (cv):\", scores_mae.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}